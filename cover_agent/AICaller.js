const axios = require('axios');
const fs = require('fs');
const path = require('path');
const moment = require('moment');
require('dotenv').config();

class AICaller {
    constructor(model, apiBase = "") {
        /**
         * Initializes an instance of the AICaller class.
         *
         * Parameters:
         *     model (string): The name of the model to be used.
         *     apiBase (string): The base API url to use in case model is set to Ollama or Hugging Face
         */
        this.model = model;
        this.apiBase = apiBase;
    }

    async callModel(prompt, maxTokens = 4096) {
        /**
         * Call the language model with the provided prompt and retrieve the response.
         *
         * Parameters:
         *     prompt (object): The prompt to be sent to the language model.
         *     maxTokens (number, optional): The maximum number of tokens to generate in the response. Defaults to 4096.
         *
         * Returns:
         *     object: An object containing the response generated by the language model, the number of tokens used from the prompt, and the total number of tokens in the response.
         */
        if (!prompt.system || !prompt.user) {
            throw new Error("The prompt object must contain 'system' and 'user' keys.");
        }

        let messages;
        if (prompt.system === "") {
            messages = [{ role: "user", content: prompt.user }];
        } else {
            messages = [
                { role: "system", content: prompt.system },
                { role: "user", content: prompt.user }
            ];
        }

        // Default Completion parameters
        const completionParams = {
            model: this.model,
            messages: messages,
            max_tokens: maxTokens,
            stream: true,
            temperature: 0.2,
        };

        // API base exception for OpenAI Compatible, Ollama and Hugging Face models
        if (this.model.includes("ollama") || this.model.includes("huggingface") || this.model.startsWith("openai/")) {
            completionParams.api_base = this.apiBase;
        }

        try {
            const response = await axios.post(`https://${this.apiBase}/v1/completions`, completionParams, {
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`
                },
                responseType: 'stream'
            });

            let chunks = [];
            console.log("Streaming results from LLM model...");
            response.data.on('data', chunk => {
                process.stdout.write(chunk.toString());
                chunks.push(chunk);
            });

            response.data.on('end', () => {
                console.log("\n");
            });

            const modelResponse = streamChunkBuilder(chunks, messages);

            if (process.env.WANDB_API_KEY) {
                const rootSpan = {
                    name: `inference_${moment().format('YYYY-MM-DD_HH-mm-ss')}`,
                    kind: "llm",
                    inputs: { user_prompt: prompt.user, system_prompt: prompt.system },
                    outputs: { model_response: modelResponse.choices[0].message.content }
                };
                logInference(rootSpan);
            }

            return {
                response: modelResponse.choices[0].message.content,
                promptTokens: parseInt(modelResponse.usage.prompt_tokens),
                responseTokens: parseInt(modelResponse.usage.completion_tokens)
            };

        } catch (error) {
            console.error(`Error during streaming: ${error.message}`);
        }
    }
}

function streamChunkBuilder(chunks, messages) {
    // Implement the logic for building the stream chunks here
    // This is a placeholder implementation, you should adapt it based on your requirements
    let content = chunks.map(chunk => chunk.toString()).join('');
    return {
        choices: [
            {
                message: {
                    content: content
                }
            }
        ],
        usage: {
            prompt_tokens: messages.reduce((acc, msg) => acc + msg.content.length, 0),
            completion_tokens: content.length
        }
    };
}

function logInference(span) {
    // Implement logging to WandB or other services here
    console.log('Logging inference:', span);
}

module.exports = AICaller;
